data:
  dataset_id: "sentence-transformers/NanoBEIR-en"
  train_ratio: 0.8

pool:
  pool_size: 256
  pos_cap: 3
  rand_count: 16
  amb_count: 0
  bm25_depth: 100
  refresh_interval: 500

model:
  encoder_name: "answerdotai/ModernBERT-base"
  hidden_size: 768
  head_output_size: 256
  pooling: "mean"
  normalize_output: false
  freeze_backbone: true

eggroll:
  rank: 1
  population_size: 256
  sigma: 0.02
  adaptive_sigma: true
  sigma_min: 0.005
  sigma_max: 0.1
  sigma_target_variance: 0.1

ndcg:
  k: 20
  eval_k: [10, 20, 100]

update:
  learning_rate: 0.05
  clip_norm: 1.0
  weight_decay: 0.0001
  momentum: 0.0
  use_ema: false
  ema_decay: 0.999

shaping:
  method: "rank"

training:
  num_steps: 5000
  eval_interval: 100
  full_eval_interval: 500
  log_interval: 10
  checkpoint_interval: 1000

infra:
  device: "cuda"
  seed: 42
  cache_dir: "./cache"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

logging:
  use_wandb: true
  project_name: "eggroll-ndcg"
  log_level: "INFO"
